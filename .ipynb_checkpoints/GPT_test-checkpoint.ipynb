{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e72fb2-ffe7-4008-bfa0-2f65fed6a4a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Settings (empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4832388-12db-46b2-8730-026a3673e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configparser\n",
    "# try:\n",
    "#    config = open(\"config.ini\")\n",
    "# except FileNotFoundError:\n",
    "#    API = input(\"Please Enter Open AI API:\")\n",
    "# else: \n",
    "#    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f36b5-8bb9-49b8-be19-b70c04b83832",
   "metadata": {
    "tags": []
   },
   "source": [
    "# API test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab6f992-6039-444d-843e-511090d46f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1006d-e942-41e7-a1c2-1866b616186a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ebbc194-1540-4b70-816b-73f89dbbe3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_abs(text,question=\"\",usage=False):\n",
    "   Require = \"Please answer the questions below without complete sentence refer to this artical\"\n",
    "   default_Qabs = '''\n",
    "         1. How many new strains have been discovered?\\n\n",
    "         2. What's the name of those new strains?\\n\n",
    "         3. What lineage does this species belong to?\\n\n",
    "         4. What the characteristics of these strains?\\n\n",
    "         5. What its best optimal growth environment?\\n\n",
    "         6. What is the metabolic profile of the strain?\\n\"\n",
    "   '''\n",
    "   if not question:\n",
    "      question = default_Qabs\n",
    "   Prompt = \"{Require}:\\n{Abstract}\\nQuestions:\\n{Question}\".format(Require = Require, Abstract = text, Question = question)\n",
    "   response = openai.Completion.create(\n",
    "     model=\"text-davinci-003\",\n",
    "     prompt=Prompt,\n",
    "     temperature=0,\n",
    "     max_tokens=200,\n",
    "     top_p=1,\n",
    "     frequency_penalty=0,\n",
    "     presence_penalty=0\n",
    "   )\n",
    "   Answer = response['choices'][0]['text']\n",
    "   pattern = r\"\\d+\\.\\s+(.*)\"\n",
    "   Answer_list = re.findall(pattern,Answer)\n",
    "   Answer_list = [i.encode(\"utf8\") for i in Answer_list]\n",
    "   if usage:\n",
    "      print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')\n",
    "   return Answer_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee71d7-7236-4afb-a035-4bce9329b8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb0456d-1acf-45f4-804b-8eb90c8027ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parse article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48fc9cd-67f5-4e7c-b361-7dd8057880c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_meta_info(url,warning = False):\n",
    "   #Send HTTP request\n",
    "   headers = {\n",
    "       'User-Agent':\n",
    "       'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "   }\n",
    "   try:\n",
    "      response = requests.get(url,headers,timeout=10)\n",
    "      if '404' in response.url:\n",
    "           raise Exception('No data found for this link: '+source_url)\n",
    "      Nopage = re.compile(r'Page Not Found')\n",
    "      if Nopage.search(response.text):\n",
    "         if warning:\n",
    "            raise Exception(\"Page Not Found\")\n",
    "\n",
    "      #Redirect the page\n",
    "      url_head = \"https://www.microbiologyresearch.org/\"\n",
    "      match_url = re.search(r'data-fullTexturl=\"(.*?)\"',response.text)\n",
    "      if match_url:\n",
    "          url_adhere = match_url.group(1)\n",
    "      elif warning:\n",
    "         raise Exception(\"Can't Redirect\")\n",
    "   except:\n",
    "      raise\n",
    "   # Parsing full-text content\n",
    "   redirect = url_head + url_adhere\n",
    "   full_text_url = requests.get(redirect,headers,timeout=10).text\n",
    "   full_text = requests.get(full_text_url,headers,timeout=10)\n",
    "   \n",
    "   # Using latin encodeing and drop menu elements\n",
    "   soup = BeautifulSoup(full_text.text.encode('latin1').decode('utf-8').replace('\\u200a','').replace('\\u2006','').replace('\\u2009',''),features= 'html.parser')\n",
    "   elements_to_remove = soup.find_all('div', {'class': 'dropDownMenu'})\n",
    "   for element in elements_to_remove:\n",
    "       element.decompose()\n",
    "   elements_to_remove = soup.find_all('div', {'class': 'menuButton'})\n",
    "   for element in elements_to_remove:\n",
    "       element.decompose()\n",
    "\n",
    "   # get meta data\n",
    "   title = soup.find('h1').text.strip()\n",
    "   abstract = soup.find('div', {'class': 'articleSection article-abstract'}).text\n",
    "   abstract = re.sub(r'\\\\u[0-9a-fA-F]{4}', '',abstract)\n",
    "   article = soup.find_all('div', {'class': 'articleSection'})\n",
    "   return {\"title\":title,\"abstract\":abstract,\"article\":article}\n",
    "\n",
    "def get_text(article):   \n",
    "   article_sorted ={\"Head\":\"\"}\n",
    "   regex_main = r'<a id=\".+\" name=\".+\">(.*?)<\\/a><\\/div>'\n",
    "   current_part = \"Head\"\n",
    "   for text in article:\n",
    "      main_match = re.search(regex_main, str(text))\n",
    "      if main_match:\n",
    "         if main_match.group(1) != current_part:\n",
    "            article_sorted[main_match.group(1)]=\"\"\n",
    "            current_part = main_match.group(1)\n",
    "            for p in text.find_all('p'):\n",
    "                article_sorted[main_match.group(1)] += p.text\n",
    "      else: \n",
    "         for p in text.find_all('p'):\n",
    "                article_sorted[current_part] += p.text\n",
    "\n",
    "   for part in article_sorted:\n",
    "      article_sorted[part] = re.sub(r'\\[.*?\\]', '',article_sorted[part])\n",
    "      article_sorted[part] = re.sub(r'\\((http\\S+)\\)', '',article_sorted[part])\n",
    "      article_sorted[part] = re.sub(r'\\(([^)]*http[^)]*)\\)', '',article_sorted[part])\n",
    "      article_sorted[part] = re.sub(r'\\\\u[0-9a-fA-F]{4}', '',article_sorted[part])\n",
    "      \n",
    "   return article_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d753f6ba-249f-40e7-a7d0-02a71d07918a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = 'https://doi.org/10.1099/ijsem.0.004430'\n",
    "# get_text(get_meta_info(url)[\"article\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b5b86-70d1-4ce0-9db9-fcda2eb5ebef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parse volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb3c634-2d23-4c4c-9bac-59fc2aff97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_taxa_list(url):\n",
    "#    url = url + \"?pageSize=100&page=1\"\n",
    "#    volume_page = requests.get(url,headers)\n",
    "#    soup = Beautifulsoup(volume_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3598fc-7502-4389-b6ca-0096162e2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volume_list():   \n",
    "   url_head = \"https://www.microbiologyresearch.org\"\n",
    "   all_volume = \"https://www.microbiologyresearch.org/content/journal/ijsem/issueslist\"\n",
    "   headers = {\n",
    "          'User-Agent':\n",
    "          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "      }\n",
    "   soup = BeautifulSoup(requests.get(all_volume,headers,timeout=10).text,'html.parser')\n",
    "   volume_list = []\n",
    "   grab_list = soup.find_all('a')\n",
    "   pattern = r'\"([^\"]*)\"'\n",
    "   for a in grab_list:\n",
    "      volume_list.append(url_head + re.search(pattern, str(a)).group(1))\n",
    "   return volume_list\n",
    "\n",
    "\n",
    "def parse_volume(url):\n",
    "   headers = {\n",
    "          'User-Agent':\n",
    "          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "      }\n",
    "   url = url + \"?pageSize=100&page=1\"\n",
    "   volume_page = requests.get(url,headers,timeout=10)\n",
    "   soup = BeautifulSoup(volume_page.text)\n",
    "   return soup\n",
    "\n",
    "def get_volume_info(soup):\n",
    "   title = soup.find(\"meta\",  attrs={\"name\": \"citation_title\"})[\"content\"]\n",
    "   date = soup.find(\"meta\",  attrs={\"name\": \"citation_date\"})[\"content\"]\n",
    "   year = soup.find(\"meta\",  attrs={\"name\": \"citation_year\"})[\"content\"]\n",
    "   info = {\"title\":title,\"date\":date,\"year\":year}\n",
    "   return info\n",
    "\n",
    "def get_atricle_list(soup):\n",
    "   article_block = soup.find('div', {'class': 'issueTocContents table-wrapper'})\n",
    "   article_block = article_block.find_all('ul', {'class': 'list-unstyled'})\n",
    "   pattern = r'<span class=\"heading1\">\\nNew Taxa\\n</span>'\n",
    "   for i in range(len(article_block)):\n",
    "      match = re.search(pattern,str(article_block[i]))\n",
    "      if match: taxa_block_pos = i\n",
    "   taxa_block = str(article_block[taxa_block_pos])\n",
    "\n",
    "   pattern_field = r'(?<=<span class=\"tocheading2\">\\n)\\w+(?=\\n<\\/span>)'\n",
    "   taxa_list = [*re.finditer(pattern_field,taxa_block)]\n",
    "   pos = [i.start() for i in taxa_list]+[len(taxa_block)]\n",
    "   taxa = [i.group() for i in taxa_list]\n",
    "   article_list = {}\n",
    "   for i in range(len(taxa)):\n",
    "      sub_block = taxa_block[pos[i]:pos[i+1]]\n",
    "      pattern_doi = r'(?<=href=\")https://doi.org/10.1099/ijsem.0.\\d+'\n",
    "      article_list[taxa[i]] = re.findall(pattern_doi,sub_block)\n",
    "   return article_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe26f4a-17a7-40bb-bfbd-01d6a48c3f99",
   "metadata": {},
   "source": [
    "# Running test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2167eb-b2d7-4487-b76f-7da10a1c6dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Volume:  Volume 73, Issue 2\n",
      "Actinomycetota : 3\n",
      "Bacteroidota : 3\n",
      "Bacillota : 4\n",
      "Pseudomonadota : 19\n",
      "(1/29)Currently parsing article:   Three novel Actinoplanes species isolated by using polyaspartic acid as a water-retaining agent for the enrichment in situ\n",
      "(2/29)Currently parsing article:   Corynebacterium megadyptis sp. nov. with two subspecies, Corynebacterium megadyptis subsp. megadyptis subsp. nov. and Corynebacterium megadyptis subsp. dunedinense subsp. nov. isolated from yellow-eyed penguins\n",
      "(3/29)Currently parsing article:   Arthrobacter mangrovi sp. nov., an actinobacterium isolated from the rhizosphere of a mangrove\n",
      "(4/29)Currently parsing article:   Muricauda spongiicola sp. nov., isolated from the sponge Callyspongia elongata\n",
      "(5/29)Currently parsing article:   Phocaeicola oris sp. nov., an anaerobic bacterium isolated from the saliva of a patient with oral squamous cell carcinoma\n",
      "(6/29)Currently parsing article:   Mucilaginibacter straminoryzae sp. nov., isolated from rice straw used for growing periphyton\n",
      "(7/29)Currently parsing article:   Acidaminococcus homini s sp. nov., Amedibacillus hominis sp. nov., Lientehia hominis gen. nov. sp. nov., Merdimmobilis hominis gen. nov. sp. nov., and Paraeggerthella hominis sp. nov., isolated from human faeces\n",
      "(8/29)Currently parsing article:   Radiobacillus kanasensis sp. nov., a halotolerant bacterium isolated from woodland soil\n",
      "(9/29)Currently parsing article:   Fructilactobacillus cliffordii sp. nov., Fructilactobacillus hinvesii sp. nov., Fructilactobacillus myrtifloralis sp. nov., Fructilactobacillus carniphilus sp. nov. and Fructobacillus americanaquae sp. nov., five novel lactic acid bacteria isolated from insects or flowers of Kangaroo Island, South Australia\n",
      "(10/29)Currently parsing article:   Aminithiophilus ramosus gen. nov., sp. nov., a sulphur-reducing bacterium isolated from a pyrite-forming enrichment culture, and taxonomic revision of the family Synergistaceae\n",
      "Read failed!\n",
      "(12/29)Currently parsing article:   Vibrio floridensis sp. nov., a novel species closely related to the human pathogen Vibrio vulnificus isolated from a cyanobacterial bloom\n",
      "Read failed!\n",
      "(14/29)Currently parsing article:   Sulfurospirillum diekertiae sp. nov., a tetrachloroethene-respiring bacterium isolated from contaminated soil\n",
      "(15/29)Currently parsing article:   Aristophania vespae gen. nov., sp. nov., isolated from wasps, is related to Bombella and Oecophyllibacter, isolated from bees and ants\n",
      "(16/29)Currently parsing article:   Pseudomonas nunensis sp. nov. isolated from a suppressive potato field in Greenland\n",
      "(17/29)Currently parsing article:   Sphingobium lignivorans sp. nov., isolated from river sediment downstream of a paper mill\n",
      "Read failed!\n",
      "(19/29)Currently parsing article:   Salipiger pentaromativorans sp. nov., a polycyclic aromatic hydrocarbon-degrading bacterium isolated from mangrove sediment\n",
      "(20/29)Currently parsing article:   Luteimonas galliterrae sp. nov., isolated from poultry farm soil\n",
      "(21/29)Currently parsing article:   Sneathiella marina sp. nov., isolated from a sea anemone in the Western Pacific Ocean\n",
      "(22/29)Currently parsing article:   Thermomonas paludicola sp. nov., isolated from a lotus wetland\n",
      "(23/29)Currently parsing article:   Aliiroseovarius subalbicans sp. nov. and Aliiroseovarius sediminis sp. nov., isolated from marine sediment\n",
      "(24/29)Currently parsing article:   Roseomonas acroporae sp. nov., isolated from coral Acropora digitifera\n",
      "Read failed!\n",
      "Read failed!\n",
      "(27/29)Currently parsing article:   Aspergillus verrucosus sp. nov., a xerophilic species isolated from house dust and honey in Japan\n",
      "(28/29)Currently parsing article:   Multiverruca sinensis gen. nov., sp. nov., a thermotolerant fungus isolated from soil in China\n",
      "(29/29)Currently parsing article:   Hannaella floricola sp. nov., a novel basidiomycetous yeast species isolated from a flower of Lantana camara in Portugal\n",
      "Successfully written to the file!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "year_limit = 2019\n",
    "volume_list = get_volume_list()\n",
    "for vol_url in volume_list[1:2]:\n",
    "   cur_vol = parse_volume(vol_url)\n",
    "   cur_vol_info = get_volume_info(cur_vol)\n",
    "   print(\"Current Volume: \",cur_vol_info[\"title\"])\n",
    "   # if int(cur_vol_info[\"year\"]) < year_limit:\n",
    "   #    break\n",
    "   cur_vol_article_list = get_atricle_list(cur_vol)\n",
    "   num_info = [print(i,\":\",len(j)) for i,j in cur_vol_article_list.items()]\n",
    "   span = sum([len(j) for j in cur_vol_article_list.values()])\n",
    "   cur_num = 0\n",
    "   with open(cur_vol_info[\"title\"]+\".csv\", 'w', newline='') as csvfile:\n",
    "      writer = csv.writer(csvfile, delimiter='\\t')\n",
    "      for region, url_list in cur_vol_article_list.items():\n",
    "         for article_url in url_list:\n",
    "            cur_num += 1\n",
    "            try:\n",
    "               meta_info = get_meta_info(article_url)\n",
    "            except:\n",
    "               print(\"Read failed!\")\n",
    "               continue\n",
    "            progess = \"({}/{})\".format(cur_num,span)\n",
    "            print(progess+\"Currently parsing article:  \",meta_info[\"title\"])\n",
    "            answer = ask_abs(meta_info[\"abstract\"])\n",
    "            newline = [article_url.encode(\"utf8\")] + [meta_info[\"title\"].encode(\"utf8\")] + answer\n",
    "            writer.writerow(newline)\n",
    "      print(\"Successfully written to the file!\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240ac7b-68ba-4b20-8ff6-5842f5985ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c0f0d-7709-4517-ad80-1235e73aff10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
